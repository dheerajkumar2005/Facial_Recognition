{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Facial Recognition Project using Siamese Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1_Jdd6k-CweL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-09-30 16:50:17.432755: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-09-30 16:50:17.613130: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-30 16:50:17.641063: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-30 16:50:17.645908: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-30 16:50:17.734712: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-09-30 16:50:18.593976: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "\n",
        "# tensorflow dependencies\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Layer, Dense, Conv2D, Flatten, Input, MaxPool2D\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting up required files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "s4RMAemRD1y1"
      },
      "outputs": [],
      "source": [
        "# setup file paths\n",
        "pos_path = os.path.join('facial recognition data', 'positive')\n",
        "neg_path = os.path.join('facial recognition data', 'negative')\n",
        "anchor_path = os.path.join('facial recognition data', 'anchors')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Jed8a-eVRsDo"
      },
      "outputs": [],
      "source": [
        "# uncompress the image dataset for negatives\n",
        "# from lfw dataset: http://vis-ww.cs.umass.edu/lfw/\n",
        " \n",
        "!tar -xf lfw.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aaxp6SW4QMqW"
      },
      "outputs": [],
      "source": [
        "# move lfw images to the correct directories\n",
        "\n",
        "# iterating over all directories in lfw (contains names and each contains certain images of that person)\n",
        "for directory in os.listdir('lfw'):\n",
        "  # iterating over all the files in any given person's folder\n",
        "  for file in os.listdir((os.path.join('lfw', directory))):\n",
        "    # defining previous path\n",
        "    prev_path = os.path.join('lfw', directory, file)\n",
        "    # creating new path\n",
        "    new_path = os.path.join(neg_path, file)\n",
        "    # replacing old path by new path, effectively moving the files\n",
        "    os.replace(prev_path, new_path)\n",
        "\n",
        "# might seem new, but is actually pretty intuitive and easy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Mc_dsaSCRggn"
      },
      "outputs": [],
      "source": [
        "# Import uuid library to generate unique image names for the images we'll create\n",
        "import uuid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating and storing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1FKMwgPqR--M"
      },
      "outputs": [],
      "source": [
        "# Create a connection to the webcam\n",
        "capture = cv2.VideoCapture(0)\n",
        "while capture.isOpened():\n",
        "  ret, frame = capture.read()\n",
        "\n",
        "  # cut frame down to 250x250 pixels\n",
        "  frame = frame[50:50+250, 200:200+250]\n",
        "\n",
        "  # collect anchor images\n",
        "  if cv2.waitKey(1) & 0XFF == ord(\"a\"):\n",
        "    # create unique file path\n",
        "    img_name = os.path.join(anchor_path, '{}.jpg'.format(uuid.uuid1()))\n",
        "    # save image\n",
        "    cv2.imwrite(img_name, frame)\n",
        "\n",
        "  # collect positive imagesqq\n",
        "  if cv2.waitKey(1) & 0XFF == ord(\"p\"):\n",
        "    # create unique file path\n",
        "    img_name = os.path.join(pos_path, '{}.jpg'.format(uuid.uuid1()))\n",
        "    # save image\n",
        "    cv2.imwrite(img_name, frame)\n",
        "    \n",
        "  # show image in screen\n",
        "  cv2.imshow(\"Image Collection\", frame)\n",
        "  if cv2.waitKey(1) & 0XFF == ord(\"q\"):\n",
        "    # waitKey(1) refreshes the window after every millisecond and returns a 32-bit integer\n",
        "    # we use & to retain only the required 8 digits (0XFF is 11111111) ord() returns unicode and that is within 0-255\n",
        "    # so pressing q will break\n",
        "    break\n",
        "\n",
        "capture.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# with all the data collected, we can begin preprocessing\n",
        "\n",
        "def preprocess(img_file_path):\n",
        "    # Read in image from file path\n",
        "    byte_img = tf.io.read_file(img_file_path)\n",
        "    # convert the image to a uint8 tensor\n",
        "    img = tf.io.decode_jpeg(byte_img)\n",
        "\n",
        "    # resize the image to 100x100\n",
        "    img = tf.image.resize(img, (100,100))\n",
        "    # scale the values\n",
        "    img /= 255\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "ename": "InvalidArgumentError",
          "evalue": "Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: facial recognition data/anchors\\\\*.jpg'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# get image directories\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m anchor \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchor_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m*.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m300\u001b[39m)\n\u001b[1;32m      4\u001b[0m positive \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mlist_files(pos_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m*.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m300\u001b[39m)\n\u001b[1;32m      5\u001b[0m negative \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mlist_files(neg_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m*.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m300\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:1320\u001b[0m, in \u001b[0;36mDatasetV2.list_files\u001b[0;34m(file_pattern, shuffle, seed, name)\u001b[0m\n\u001b[1;32m   1313\u001b[0m condition \u001b[38;5;241m=\u001b[39m math_ops\u001b[38;5;241m.\u001b[39mgreater(array_ops\u001b[38;5;241m.\u001b[39mshape(matching_files)[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   1314\u001b[0m                              name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch_not_empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1316\u001b[0m message \u001b[38;5;241m=\u001b[39m math_ops\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo files matched pattern: \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1318\u001b[0m     string_ops\u001b[38;5;241m.\u001b[39mreduce_join(file_pattern, separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1320\u001b[0m assert_not_empty \u001b[38;5;241m=\u001b[39m \u001b[43mcontrol_flow_assert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAssert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43massert_not_empty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcontrol_dependencies([assert_not_empty]):\n\u001b[1;32m   1323\u001b[0m   matching_files \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39midentity(matching_files)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/control_flow_assert.py:102\u001b[0m, in \u001b[0;36mAssert\u001b[0;34m(condition, data, summarize, name)\u001b[0m\n\u001b[1;32m    100\u001b[0m     xs \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_n_to_tensor(data)\n\u001b[1;32m    101\u001b[0m     data_str \u001b[38;5;241m=\u001b[39m [_summarize_eager(x, summarize) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m xs]\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError(\n\u001b[1;32m    103\u001b[0m         node_def\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    104\u001b[0m         op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    105\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to be true. Summarized data: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    106\u001b[0m         (condition, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(data_str)))\n\u001b[1;32m    107\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssert\u001b[39m\u001b[38;5;124m\"\u001b[39m, [condition, data]) \u001b[38;5;28;01mas\u001b[39;00m name:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: facial recognition data/anchors\\\\*.jpg'"
          ]
        }
      ],
      "source": [
        "# get image directories\n",
        "\n",
        "anchor = tf.data.Dataset.list_files(anchor_path+'\\*.jpg').take(300)\n",
        "positive = tf.data.Dataset.list_files(pos_path+'\\*.jpg').take(300)\n",
        "negative = tf.data.Dataset.list_files(neg_path+'\\*.jpg').take(300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# testing if data is loaded fine or not\n",
        "dir_test = anchor.as_numpy_iterator()\n",
        "print(dir_test.next())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create labelled dataset\n",
        "positives = tf.data.Dataset.zip((anchor, positive))\n",
        "y_positives=tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))\n",
        "negatives = tf.data.Dataset.zip((anchor, negative))\n",
        "y_negatives = tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))\n",
        "data = positives.concatenate(negatives)\n",
        "labels = y_positives.concatenate(y_negatives)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# verify\n",
        "samples = data.as_numpy_iterator()\n",
        "print(samples.next())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# write a new function which does the preeprocessing for a column\n",
        "def preprocess_twin(t, v):\n",
        "    return (preprocess(t), preprocess(v))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pipeline to preprocess the data\n",
        "\n",
        "data = data.map(preprocess_twin)\n",
        "dataset = tf.data.Dataset.zip((data, labels))\n",
        "dataset = dataset.cache()     # to load the files onto a cache for faster access\n",
        "dataset = dataset.shuffle(buffer_size=1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Partitioning into test and train\n",
        "\n",
        "# Training Partition\n",
        "data_train = dataset.take(round(len(data)*0.75))\n",
        "# Make batches of 16\n",
        "data_train = data_train.batch(16)\n",
        "# Preprocess 8 batches while previous ones are being trained\n",
        "data_train = data_train.prefetch(8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing Partition\n",
        "data_test = dataset.skip(round(len(data)*0.75))\n",
        "data_test = data_test.batch(16)\n",
        "data_test = data_test.prefetch(8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# defining  the distance layer\n",
        "\n",
        "class L1Dist(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        # kwargs are there to allow any other parameters such as input_dim, units etc\n",
        "        # see keras documentation of creating a custom layer\n",
        "        super().__init__()\n",
        "    \n",
        "    # the call function of the class is what the model inhernetly uses\n",
        "    def call(self, input_embedding, validation_embedding):\n",
        "        return tf.math.abs(input_embedding - validation_embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# defining the cnn embeddings\n",
        "\n",
        "cnn = Sequential([\n",
        "    Conv2D(filters=64, kernel_size=(10,10), activation='relu', input_shape=(100,100,3)),\n",
        "    MaxPool2D(64, (2,2), padding='same'),\n",
        "\n",
        "    Conv2D(filters=128, kernel_size=(7,7), activation='relu'),\n",
        "    MaxPool2D(64, (2,2), padding='same'),\n",
        "\n",
        "    Conv2D(filters=128, kernel_size=(4,4), activation='relu'),\n",
        "    MaxPool2D(64, (2,2), padding='same'),\n",
        "\n",
        "    Conv2D(filters=256, kernel_size=(4,4), activation='relu'),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(4096, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# input for the cnn\n",
        "input_image = Input(name='input_img', shape=(100,100,3))\n",
        "validation_image = Input(name='validation_img', shape=(100,100,3))\n",
        "\n",
        "# creating the siamese layer\n",
        "distance_layer = L1Dist()\n",
        "distances = distance_layer(cnn(input_image), cnn(validation_image))\n",
        "\n",
        "# creating the final layer\n",
        "output = Dense(1, activation='sigmoid')(distances)\n",
        "\n",
        "# Defining the model\n",
        "model = Model(inputs=[input_image, validation_image], outputs=output, name='siameseNetwork')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compiling the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.fit(data_train, batch_size=16, epochs=3, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# accuracy for the test data\n",
        "loss, accuracy = model.evaluate(data_test)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save(\"face recog model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model('face recog model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualising Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_data = data_test.as_numpy_iterator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img_input, img_val = run_data.next()[0]\n",
        "y_hat = model.predict([img_input, img_val])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# post-processing\n",
        "[1 if prediction > 0.5 else 0 for prediction in y_hat]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18,8))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(img_input[8])\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(img_val[8])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
